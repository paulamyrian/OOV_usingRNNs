{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOPEuCccIO0mF29TInpPqTH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"38oByWrMraDs"},"outputs":[],"source":["import numpy\n","from numpy import array\n","import spacy\n","from spacy.vocab import Vocab\n","import keras as k\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential, load_model\n","from keras.layers import GRU, Embedding, Dense\n","import pickle\n","import nltk\n","#nltk.download('stopwords')"]},{"cell_type":"markdown","source":["Load Data"],"metadata":{"id":"0qFmDT8jsx_h"}},{"cell_type":"code","source":["data = open('data.csv').read()[:100000]"],"metadata":{"id":"kufuhWCyriTV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepare sequences for training"],"metadata":{"id":"0CB3Jz8us3n7"}},{"cell_type":"code","source":["def data_sequencing(data):   \n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts([data])\n","    with open('tokenizer.pkl', 'wb') as f:\n","        pickle.dump(tokenizer, f)\n","\n","    encoded = tokenizer.texts_to_sequences([data])[0]\n","    vocab_size = len(tokenizer.word_index) + 1\n","    #print('Vocabulary Size: %d' % vocab_size)\n","    \n","    sequences = list()\n","    \n","    for line in data.split('.'):\n","        encoded = tokenizer.texts_to_sequences([line])[0]\n","        for i in range(1, len(encoded)):\n","            sequence = encoded[:i+1]\n","            sequences.append(sequence)\n","    #print('Total Sequences: %d' % len(sequences))\n","    \n","    max_length = max([len(seq) for seq in sequences])\n","    with open('max_length.pkl', 'wb') as f:\n","        pickle.dump(max_length, f)\n","    #print('Max Sequence Length: %d' % max_length)\n","\n","    sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n","    sequences = array(sequences)\n","    X, y = sequences[:,:-1],sequences[:,-1]\n","    \n","\n","    return X,y,max_length,vocab_size"],"metadata":{"id":"2rtXPnV5rlnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X,y,max_length,vocab_size = data_sequencing(data)"],"metadata":{"id":"bzgCUOtzrsyZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define Model"],"metadata":{"id":"PzeVR_dGs7AM"}},{"cell_type":"code","source":["#https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\n","#https://keras.io/api/layers/activation_layers/\n","#https://keras.io/guides/sequential_model/\n","#https://keras.io/api/layers/recurrent_layers/\n","#https://keras.io/api/layers/core_layers/embedding/\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size,100, input_length=max_length-1))\n","model.add(LSTM(100))\n","model.add(Dense(vocab_size, activation='softmax'))\n","print(model.summary())"],"metadata":{"id":"8NrNRBSPrwDN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train Model"],"metadata":{"id":"5JvdZY_Hs9C4"}},{"cell_type":"code","source":["#https://keras.io/api/metrics/\n","#https://keras.io/api/models/\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X, y,batch_size=100, epochs=50, verbose=2)\n","model.save('grumodel.h5')"],"metadata":{"id":"Y00KmuZ-sd-L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generate sequences using model"],"metadata":{"id":"OC2VNFaatJA4"}},{"cell_type":"code","source":["def generate_seq(model, tokenizer, max_length, seed_text):\n","    if seed_text == \"\":\n","        return \"\"\n","    else:\n","        in_text = seed_text\n","        n_words = 1\n","        n_preds = 10\n","        pred_words = \"\"\n","        for _ in range(n_words):\n","            encoded = tokenizer.texts_to_sequences([in_text])[0]\n","            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n","            proba = model.predict(encoded, verbose=0).flatten()\n","            yhat = numpy.argsort(-proba)[:n_preds] \n","            out_word = ''\n","\n","            for _ in range(n_preds):\n","                for word, index in tokenizer.word_index.items():\n","                    if index == yhat[_] and word not in stopwords:\n","                        out_word = word\n","                        pred_words += ' ' + out_word\n","                        #print(out_word)\n","                        break\n","\n","\n","        return pred_words"],"metadata":{"id":"ua18c0kCshlq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load training model and SpaCy Model (Glove)"],"metadata":{"id":"4af1Xu-KtbzB"}},{"cell_type":"code","source":["model = load_model('grumodel.h5')\n","\n","with open('tokenizer.pkl', 'rb') as f:\n","    tokenizer = pickle.load(f)\n","    \n","with open('max_length.pkl', 'rb') as f:\n","    max_length = pickle.load(f)\n","\n","stopwords = nltk.corpus.stopwords.words('portuguese')\n","\n","#https://spacy.io/models/pt\n","nlp = spacy.load('pt_core_news_md')"],"metadata":{"id":"31JBe9KHspDD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function to set embeddings for OOV"],"metadata":{"id":"b62a2Pr6tkru"}},{"cell_type":"code","source":["def set_embedding_for_oov(doc):\n","    for token in doc:\n","        if token.is_oov == True:\n","            before_text = doc[:token.i].text\n","\n","            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n","            \n","            embedding = numpy.zeros((300,))\n","\n","            i=len(before_text)\n","            print('Words predicted from forward sequence model:')\n","            for word in pred_before:\n","                print(word)\n","                embedding += i*nlp.vocab.get_vector(word)\n","                i= i*.5\n","\n","            nlp.vocab.set_vector(token.text, embedding)\n","            print(token.text,nlp.vocab.get_vector(token.text)) "],"metadata":{"id":"gtbpJUWpsrd8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Detect OOV"],"metadata":{"id":"bYfWfYbjuX79"}},{"cell_type":"code","source":["nlp.vocab.get_vector('banzeiro')"],"metadata":{"id":"UiPGUtXWuUS_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Verify OOV using function"],"metadata":{"id":"wkPryr1SttwD"}},{"cell_type":"code","source":["doc1 = nlp('O Barco atravessou o banzeiro sem sofrer danos')\n","set_embedding_for_oov(doc1)"],"metadata":{"id":"cnSbOOJ2ucOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Verify similarity"],"metadata":{"id":"xu1zRRnBu0KU"}},{"cell_type":"code","source":["#BANZEIRO – Onda do mar\n","\n","doc2 = nlp('Senhor, a perseverança das ondas do mar, que fazem de cada recuo um ponto de partida para um novo avanço.')\n","\n","doc3 = nlp('As vezes é preciso ser como as ondas do mar, recuar para ganhar força.')\n","\n","doc4 = nlp('Como ondas do mar')"],"metadata":{"id":"R4mh0CBbuwGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(doc1, \"<->\", doc2, \"=\", doc1.similarity(doc2))\n","print(doc1, \"<->\", doc3, \"=\", doc1.similarity(doc3))\n","print(doc1, \"<->\", doc4, \"=\",doc1.similarity(doc4))"],"metadata":{"id":"8JBq_FO_uxC3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Verify POS tagging"],"metadata":{"id":"2I4kVKt-u3hA"}},{"cell_type":"code","source":["for token in doc1:\n","    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n","            token.shape_)"],"metadata":{"id":"W5jO-q1QuzJc"},"execution_count":null,"outputs":[]}]}